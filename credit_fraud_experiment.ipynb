{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.utils_train import train_supervised, train_models_in_threads, test_model_in_batches\n",
    "from utils.utils_plots import plot_first_feature\n",
    "from utils.utils_dataset import balance_dataset, prepare_dataset\n",
    "from utils.utils_dataset import prepare_non_iid_dataset, plot_dataset_split, display_dataset_split\n",
    "from utils.utils_metrics import calculate_metrics, plot_confusion_matrix, calculate_roc_auc\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.eGauss_plus import eGAUSSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Datasets/creditcard.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select the columns to normalize - all except 'Class'\n",
    "cols_to_normalize = [col for col in data.columns if col != 'Class']\n",
    "\n",
    "# Apply the normalization\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "local_model_params = {\n",
    "    \"feature_dim\": 30,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 2,\n",
    "    \"num_sigma\": 1,\n",
    "    \"kappa_join\": 0.6,\n",
    "    \"S_0\": 1e-10,\n",
    "    \"N_r\": 10,\n",
    "    \"c_max\": 100,\n",
    "    \"num_samples\": 200,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "federated_model_params = {\n",
    "    \"feature_dim\": 30,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 2,\n",
    "    \"num_sigma\": 1,\n",
    "    \"kappa_join\": 0.6,\n",
    "    \"S_0\": 1e-10,\n",
    "    \"N_r\": 10,\n",
    "    \"c_max\": 100,\n",
    "    \"num_samples\": 200,\n",
    "    \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display_dataset_split(client_train, test_data)\n",
    "#plot_dataset_split(client_train, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cProfile\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2):\n",
    "    differences = []\n",
    "\n",
    "    # Function to find differing indices within the overlapping range\n",
    "    def find_differing_indices(tensor1, tensor2):\n",
    "        min_length = min(tensor1.size(0), tensor2.size(0))\n",
    "        differing = (tensor1[:min_length] != tensor2[:min_length]).nonzero(as_tuple=False)\n",
    "        if differing.nelement() == 0:\n",
    "            return \"No differences\"\n",
    "        else:\n",
    "            return differing.view(-1).tolist()  # Flatten and convert to list\n",
    "\n",
    "    # Compare mu parameter and find differing indices\n",
    "    mu_equal = torch.equal(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "    if not mu_equal:\n",
    "        differing_indices_mu = find_differing_indices(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "        differences.append(f\"mu parameter differs at indices {differing_indices_mu}\")\n",
    "\n",
    "    # Compare S parameter and find differing indices\n",
    "    S_equal = torch.equal(model1.S[:model1.c], model2.S[:model2.c])\n",
    "    if not S_equal:\n",
    "        differing_indices_S = find_differing_indices(model1.S[:model1.c], model2.S[:model2.c])\n",
    "        differences.append(f\"S parameter differs at indices {differing_indices_S}\")\n",
    "\n",
    "    # Compare n parameter and find differing indices\n",
    "    n_equal = torch.equal(model1.n[:model1.c], model2.n[:model2.c])\n",
    "    if not n_equal:\n",
    "        differing_indices_n = find_differing_indices(model1.n[:model1.c], model2.n[:model2.c])\n",
    "        differences.append(f\"n parameter differs at indices {differing_indices_n}\")\n",
    "\n",
    "    # Check if there are any differences\n",
    "    if differences:\n",
    "        difference_str = \", \".join(differences)\n",
    "        return False, f\"Differences found in: {difference_str}\"\n",
    "    else:\n",
    "        return True, \"Models are identical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def run_experiment(num_clients, num_rounds, clients_data, test_data):\n",
    "        \n",
    "    # Initialize a model for each client\n",
    "    local_models = [eGAUSSp(**local_model_params) for _ in range(num_clients)]\n",
    "    federated_model = eGAUSSp(**federated_model_params)\n",
    "\n",
    "    # Initialize a list to store the metrics for each round\n",
    "    round_metrics = []\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "        print(f\"--- Communication Round {round + 1} ---\")\n",
    "\n",
    "        aggregated_model = eGAUSSp(**federated_model_params)\n",
    "        federated_model = eGAUSSp(**federated_model_params)\n",
    "\n",
    "        # Train local models\n",
    "        train_models_in_threads(local_models, clients_data)\n",
    "        '''\n",
    "        for local_model, client_data in zip(local_models, clients_data):\n",
    "             train_supervised(local_model, client_data)\n",
    "\n",
    "             print(f\"Number of local model clusters = {sum(local_model.n[0:local_model.c]> local_model.kappa_n)}\")\n",
    "             all_scores, pred_max, _ = test_model_in_batches(local_model, client_data)\n",
    "             binary = calculate_metrics(pred_max, client_data, \"binary\")\n",
    "             roc_auc = calculate_roc_auc(all_scores, client_data)\n",
    "             print(f\"Test Metrics: {binary}\")\n",
    "             print(f\"Test ROC AUC: {roc_auc}\")\n",
    "             plot_confusion_matrix(pred_max, client_data)\n",
    "        '''   \n",
    "        # Update federated model with local models\n",
    "        for client_idx, client_model in enumerate(local_models):\n",
    "            #client_model.federal_agent.federated_merging()\n",
    "            print(f\"Updating agreggated model with client {client_idx + 1}\")\n",
    "        \n",
    "            aggregated_model.federal_agent.merge_model_privately(client_model, client_model.kappa_n)\n",
    "            print(f\"Number of agreggated clusters after transfer = {sum(aggregated_model.n[0:aggregated_model.c]> aggregated_model.kappa_n)}\")\n",
    "                \n",
    "            aggregated_model.federal_agent.federated_merging()\n",
    "            print(f\"Number of agreggated clusters after merging = {sum(aggregated_model.n[0:aggregated_model.c]> aggregated_model.kappa_n)}\")\n",
    "\n",
    "                \n",
    "        #client_model.score = 0*client_model.score  \n",
    "        #aggregated_model.S_glo = client_model.S_glo\n",
    "        #aggregated_model.mu_glo = client_model.mu_glo     \n",
    "        '''\n",
    "        if round>1:\n",
    "            with torch.no_grad():\n",
    "                aggregated_model.S = nn.Parameter(aggregated_model.S/2)\n",
    "                aggregated_model.n = nn.Parameter(aggregated_model.n/2)\n",
    "\n",
    "                aggregated_model.S_glo = aggregated_model.S_glo/2\n",
    "                aggregated_model.n_glo = aggregated_model.n_glo/2\n",
    "        '''\n",
    "\n",
    "        # New code for comparison using the updated compare_models function\n",
    "        #are_models_same, comparison_message = compare_models(client_model, aggregated_model)\n",
    "        #print(f\"Comparison details: {comparison_message}\")\n",
    "\n",
    "        # Update federated model with local models\n",
    "        print(f\"Updating federated model with agreggated model\")\n",
    "        federated_model = aggregated_model #.federal_agent.merge_model_privately(aggregated_model, federated_model.kappa_n)\n",
    "        print(f\"Number of federated clusters after transfer = {sum(federated_model.n[0:federated_model.c]> federated_model.kappa_n)}\")\n",
    "\n",
    "        local_models = [eGAUSSp(**local_model_params) for _ in range(num_clients)]  \n",
    "        \n",
    "        # Perform federated merging and removal mechanism on the federated model\n",
    "        if any(federated_model.n[0:federated_model.c]> federated_model.kappa_n):\n",
    "\n",
    "            # Evaluate federated model\n",
    "            all_scores_fed, pred_max_fed, _ = test_model_in_batches(federated_model, test_data, batch_size=1000)\n",
    "            binary_fed = calculate_metrics(pred_max_fed, test_data, \"binary\")\n",
    "            roc_auc_fed = calculate_roc_auc(all_scores_fed, test_data)\n",
    "            print(f\"Test Metrics: {binary_fed}\")\n",
    "            print(f\"Test ROC AUC: {roc_auc_fed}\")\n",
    "\n",
    "            plot_confusion_matrix(pred_max_fed, test_data)\n",
    "\n",
    "            # Append metrics to the list\n",
    "            round_metrics.append({\n",
    "                'round': round + 1,\n",
    "                'clusters': federated_model.c,\n",
    "                'binary': binary_fed,\n",
    "                'roc_auc': roc_auc_fed\n",
    "            })\n",
    "            \n",
    "\n",
    "        # Return the updated federated model to each client\n",
    "        for client_idx in range(len(local_models)):\n",
    "            print(f\"Returning updated model to client {client_idx + 1}\")\n",
    "            #local_models[client_idx] = federated_model\n",
    "            \n",
    "            local_models[client_idx].federal_agent.merge_model_privately(federated_model, federated_model.kappa_n)\n",
    "            #local_models[client_idx].score = torch.zeros_like(local_models[client_idx].score)\n",
    "            #local_models[client_idx].num_pred = torch.zeros_like(local_models[client_idx].score)\n",
    "                      \n",
    "            '''\n",
    "            # Return the updated federated model to each client\n",
    "            for client_idx, client_model in enumerate(local_models):\n",
    "                print(f\"Returning updated model to client {client_idx + 1}\")\n",
    "                client_model.federal_agent.merge_model_privately(federated_model, federated_model.kappa_n)\n",
    "                client_model.federal_agent.federated_merging()\n",
    "            '''\n",
    "            \n",
    "        print(f\"--- End of Round {round + 1} ---\\n\")\n",
    "        if  round == (num_rounds-1):\n",
    "            plot_first_feature(test_data, model=federated_model, num_sigma=2, N_max = federated_model.kappa_n)   \n",
    "    \n",
    "    # After all rounds\n",
    "    print(\"All Rounds Completed. Metrics Collected:\")\n",
    "    for metric in round_metrics:\n",
    "        print(f\"Round {metric['round']}: Metrics: {metric['binary']}, ROC AUC: {metric['roc_auc']}\")\n",
    "        #print(f\"                         Weighted: {metric['weighted']}\")\n",
    "\n",
    "\n",
    "    return round_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: {0: 22746, 1: 22746}\n",
      "Client 2: {0: 22745, 1: 22745}\n",
      "Client 3: {0: 22745, 1: 22745}\n",
      "Client 4: {0: 22745, 1: 22745}\n",
      "Client 5: {0: 22745, 1: 22745}\n",
      "Client 6: {0: 22745, 1: 22745}\n",
      "Client 7: {0: 22745, 1: 22745}\n",
      "Client 8: {0: 22745, 1: 22745}\n",
      "Client 9: {0: 22745, 1: 22745}\n",
      "Client 10: {0: 22745, 1: 22745}\n",
      "Test Set: {0: 56864, 1: 98}\n",
      "\n",
      "Combined Number of Samples per Class:\n",
      "Class 0: 284315 samples\n",
      "Class 1: 227549 samples\n",
      "\n",
      "Total Number of Samples Across All Datasets: 511864\n",
      "Running experiment with 10 clients and data configuration 3\n",
      "--- Communication Round 1 ---\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n",
      "Debugging has been disabled.\n",
      "Evolving has been enabled.\n"
     ]
    }
   ],
   "source": [
    "# List of client counts and data configuration indices\n",
    "client_counts = [10, 3]\n",
    "data_config_indices = [3, 1]  # Replace with your actual data configuration indices\n",
    "\n",
    "# Assuming local_models, client_train, federated_model, and test_data are already defined\n",
    "# Number of communication rounds\n",
    "num_rounds = 10\n",
    "profiler = False\n",
    "experiments = []\n",
    "# Running the experiment\n",
    "for num_clients in client_counts:\n",
    "    for data_config_index in data_config_indices:\n",
    "        if data_config_index == 1:\n",
    "            X = data.iloc[:, :-1].values\n",
    "            y = data.iloc[:, -1].values\n",
    "            client_train, test_data, all_data = prepare_dataset(X, y, num_clients, balance = 10000) \n",
    "            #'random', 'centroids', 'nearmiss', 'enn', 'smote', int num of samples\n",
    "            \n",
    "        if data_config_index == 3:\n",
    "            X = data.iloc[:, :-1].values\n",
    "            y = data.iloc[:, -1].values\n",
    "            client_train, test_data, all_data = prepare_dataset(X, y, num_clients, balance = 'smote') \n",
    "\n",
    "        display_dataset_split(client_train, test_data)\n",
    "        \n",
    "        print(f\"Running experiment with {num_clients} clients and data configuration {data_config_index}\")\n",
    "        if profiler:\n",
    "            print(f\"... with profiler\")\n",
    "            pr = cProfile.Profile()\n",
    "            pr.enable()\n",
    "            metrics =  run_experiment(num_clients, num_rounds, client_train, test_data)\n",
    "            pr.disable()\n",
    "            pr.print_stats(sort='cumtime')\n",
    "            \n",
    "        else:\n",
    "            metrics = run_experiment(num_clients, num_rounds, client_train, test_data)\n",
    "            experiments.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Rounds Completed. Metrics Collected:\")\n",
    "for metric in metrics:\n",
    "    print(f\"Round {metric['round']}: Metrics: {metric['binary']}, ROC AUC: {metric['roc_auc']}\")\n",
    "    print(f\"                         Weighted: {metric['weighted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for client_idx, client_model in enumerate(local_models):\n",
    "        print(f\"Merging client {client_idx + 1}\")\n",
    "        #print(f\"Number of client {client_idx + 1} clusters before merging = {torch.sum(client_model.n[:client_model.c]>client_model.kappa_n)}\")\n",
    "        #client_model.federal_agent.federated_merging() \n",
    "        print(f\"Number of client {client_idx + 1} after merging = {torch.sum(client_model.n[:client_model.c]>client_model.kappa_n)}\")\n",
    "        federated_model.federal_agent.merge_model_privately(client_model, client_model.kappa_n)\n",
    "\n",
    "print(f\"Number of clusters after transfer = {federated_model.c}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "federated_model.federal_agent.federated_merging()\n",
    "federated_model.removal_mech.removal_mechanism()\n",
    "print(f\"Number of clusters after merging = {federated_model.c}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "print(f\"\\nTesting federated model\")   \n",
    "\n",
    "all_scores, pred_max, _ = test_model(federated_model, test_data)\n",
    "metrics = calculate_metrics(pred_max, test_data, \"binary\")\n",
    "print(f\"Test Metrics: {metrics}\")\n",
    "roc_auc = calculate_roc_auc(all_scores, test_data)\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "\n",
    "plot_confusion_matrix(pred_max, test_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Confusion matrix values\n",
    "tn = 135\n",
    "fn = 10\n",
    "tp = 132\n",
    "fp = 19\n",
    "\n",
    "# Creating the confusion matrix\n",
    "y_true = [0]*tn + [1]*fn + [1]*tp + [0]*fp  # 0 for negative class, 1 for positive class\n",
    "y_pred = [0]*(tn+fn) + [1]*(fp+tp)  # Predictions\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(accuracy, precision, recall, f1)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
