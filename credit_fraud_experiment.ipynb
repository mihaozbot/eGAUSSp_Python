{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.utils_train import train_supervised, train_models_in_threads, test_model_in_batches\n",
    "from utils.utils_plots import plot_first_feature, plot_interesting_features\n",
    "from utils.utils_dataset import balance_dataset, prepare_dataset\n",
    "from utils.utils_dataset import prepare_non_iid_dataset, plot_dataset_split, display_dataset_split\n",
    "from utils.utils_metrics import calculate_metrics, plot_confusion_matrix, calculate_roc_auc\n",
    "#%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.eGauss_plus import eGAUSSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Datasets/creditcard.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select the columns to normalize - all except 'Class'\n",
    "cols_to_normalize = [col for col in data.columns if col != 'Class']\n",
    "\n",
    "# Apply the normalization\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import threading  # Import the threading module\n",
    "\n",
    "if False:\n",
    "\n",
    "    num_clients = 1\n",
    "\n",
    "    # Define the range of values for each parameter\n",
    "    num_sigma_values = [5, 10, 12, 20]\n",
    "    kappa_join_values = [0.3, 0.5, 0.7, 0.8]\n",
    "    N_r_values = [10, 12, 16, 20, 30]\n",
    "\n",
    "    # Total number of experiments\n",
    "    total_experiments = len(num_sigma_values) * len(kappa_join_values) * len(N_r_values)\n",
    "    completed_experiments = 0\n",
    "\n",
    "    # Define other parameters and data setup\n",
    "    local_model_params = {\n",
    "        \"feature_dim\": 30,\n",
    "        \"num_classes\": 2,\n",
    "        \"kappa_n\": 1,\n",
    "        \"S_0\": 1e-10,\n",
    "        \"c_max\": 100,\n",
    "        \"num_samples\": 200, \n",
    "        \"device\": device  # Make sure 'device' is defined\n",
    "    }\n",
    "\n",
    "    # Placeholder for the best parameters and best score\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "\n",
    "    # List to store all results\n",
    "    results = []\n",
    "\n",
    "    # Function to write data to a file\n",
    "    def write_to_file(file_path, data, mode='a'):\n",
    "        with open(file_path, mode) as file:\n",
    "            file.write(data + \"\\n\")\n",
    "\n",
    "    # Prepare the dataset\n",
    "    # Assuming prepare_dataset function and data are defined\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    client_train, test_data, all_data = prepare_dataset(X, y, num_clients, balance=\"centroids\")\n",
    "\n",
    "    # Initialize a lock and a shared variable for progress tracking\n",
    "    lock = threading.Lock()\n",
    "    completed_experiments = 0\n",
    "    total_experiments = len(num_sigma_values) * len(kappa_join_values) * len(N_r_values)\n",
    "\n",
    "    # Function to execute model training and evaluation\n",
    "    def train_evaluate_model(params):\n",
    "        global completed_experiments\n",
    "        \n",
    "        num_sigma, kappa_join, N_r = params\n",
    "        local_model_params.update({\"num_sigma\": num_sigma, \"kappa_join\": kappa_join, \"N_r\": N_r})\n",
    "\n",
    "        local_model = eGAUSSp(**local_model_params)\n",
    "        train_supervised(local_model, client_train[0])\n",
    "\n",
    "        _, pred_max, _ = test_model_in_batches(local_model, test_data, batch_size = 1000)\n",
    "        metrics = calculate_metrics(pred_max, test_data, weight=\"binary\")\n",
    "        f1_score = metrics[\"f1_score\"]\n",
    "\n",
    "        result_str = f\"num_sigma={num_sigma}, kappa_join={kappa_join}, N_r={N_r}, F1 Score: {f1_score}\"\n",
    "        print(result_str)\n",
    "        write_to_file(\"experiment_results.txt\", result_str)  # Write results to file\n",
    "        \n",
    "        # Update progress\n",
    "        with lock:\n",
    "            completed_experiments += 1\n",
    "            progress = (completed_experiments / total_experiments) * 100\n",
    "            print(f\"Progress: {completed_experiments}/{total_experiments} ({progress:.2f}%)\")\n",
    "\n",
    "        return {\"num_sigma\": num_sigma, \"kappa_join\": kappa_join, \"N_r\": N_r, \"f1_score\": f1_score}\n",
    "        \n",
    "\n",
    "    # Write initial setup data to file\n",
    "    initial_setup_str = f\"Initial Setup: num_clients={num_clients}, num_sigma_values={num_sigma_values}, kappa_join_values={kappa_join_values}, N_r_values={N_r_values}\"\n",
    "    write_to_file(\"experiment_results.txt\", initial_setup_str, mode='w')  # 'w' to overwrite if exists\n",
    "\n",
    "    # Using ThreadPoolExecutor to run in multiple threads\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        param_combinations = list(itertools.product(num_sigma_values, kappa_join_values, N_r_values))\n",
    "        results = list(executor.map(train_evaluate_model, param_combinations))\n",
    "\n",
    "    # Find best parameters and score\n",
    "    best_result = max(results, key=lambda x: x[\"f1_score\"])\n",
    "    best_score = best_result[\"f1_score\"]\n",
    "    best_params = {k: best_result[k] for k in [\"num_sigma\", \"kappa_join\", \"N_r\"]}\n",
    "\n",
    "    # After completing all experiments, print final results\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "'''\n",
    "    # Creating a 3D scatter plot\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=results_df['num_sigma'],\n",
    "        y=results_df['kappa_join'],\n",
    "        z=results_df['N_r'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=results_df['f1_score'],  # Set color to the F1 scores\n",
    "            colorscale='Viridis',  # Choose a colorscale\n",
    "            opacity=0.8,\n",
    "            colorbar=dict(title='F1 Score')\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    # Adding labels and title\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='num_sigma',\n",
    "            yaxis_title='kappa_join',\n",
    "            zaxis_title='N_r'\n",
    "        ),\n",
    "        title='F1 Score for Different Parameter Combinations'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "local_model_params = {\n",
    "    \"feature_dim\": 30,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 1,\n",
    "    \"num_sigma\": 10,\n",
    "    \"kappa_join\": 0.5,\n",
    "    \"S_0\": 1e-10,\n",
    "    \"N_r\": 20,\n",
    "    \"c_max\": 300,\n",
    "    \"num_samples\": 1000,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "federated_model_params = {\n",
    "    \"feature_dim\": 30,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 1,\n",
    "    \"num_sigma\": 10,\n",
    "    \"kappa_join\": 0.5,\n",
    "    \"S_0\": 1e-10,\n",
    "    \"N_r\": 20,\n",
    "    \"c_max\": 1000,\n",
    "    \"num_samples\": 1000,\n",
    "    \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display_dataset_split(client_train, test_data)\n",
    "#plot_dataset_split(client_train, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2):\n",
    "    differences = []\n",
    "\n",
    "    # Function to find differing indices within the overlapping range\n",
    "    def find_differing_indices(tensor1, tensor2):\n",
    "        min_length = min(tensor1.size(0), tensor2.size(0))\n",
    "        differing = (tensor1[:min_length] != tensor2[:min_length]).nonzero(as_tuple=False)\n",
    "        if differing.nelement() == 0:\n",
    "            return \"No differences\"\n",
    "        else:\n",
    "            return differing.view(-1).tolist()  # Flatten and convert to list\n",
    "\n",
    "    # Compare mu parameter and find differing indices\n",
    "    mu_equal = torch.equal(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "    if not mu_equal:\n",
    "        differing_indices_mu = find_differing_indices(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "        differences.append(f\"mu parameter differs at indices {differing_indices_mu}\")\n",
    "\n",
    "    # Compare S parameter and find differing indices\n",
    "    S_equal = torch.equal(model1.S[:model1.c], model2.S[:model2.c])\n",
    "    if not S_equal:\n",
    "        differing_indices_S = find_differing_indices(model1.S[:model1.c], model2.S[:model2.c])\n",
    "        differences.append(f\"S parameter differs at indices {differing_indices_S}\")\n",
    "\n",
    "    # Compare n parameter and find differing indices\n",
    "    n_equal = torch.equal(model1.n[:model1.c], model2.n[:model2.c])\n",
    "    if not n_equal:\n",
    "        differing_indices_n = find_differing_indices(model1.n[:model1.c], model2.n[:model2.c])\n",
    "        differences.append(f\"n parameter differs at indices {differing_indices_n}\")\n",
    "\n",
    "    # Check if there are any differences\n",
    "    if differences:\n",
    "        difference_str = \", \".join(differences)\n",
    "        return False, f\"Differences found in: {difference_str}\"\n",
    "    else:\n",
    "        return True, \"Models are identical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_path, data, mode='a'):\n",
    "    with open(file_path, mode) as file:\n",
    "        file.write(data + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def run_experiment(num_clients, num_rounds, client_raw_data, test_data, balance):\n",
    "       \n",
    "    # Initialize a model for each client\n",
    "    local_models = [eGAUSSp(**local_model_params) for _ in range(num_clients)]\n",
    "    federated_model = eGAUSSp(**federated_model_params)\n",
    "\n",
    "    # Initialize a list to store the metrics for each round\n",
    "    round_metrics = []\n",
    "    result_file = \"experiment_results.txt\"\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "        print(f\"--- Communication Round {round + 1} ---\")\n",
    "        round_info = f\"--- Communication Round {round + 1} ---\\n\"\n",
    "\n",
    "        # Function to balance data for a single client\n",
    "        def balance_client_data(client_data):\n",
    "            client_X, client_y = client_data\n",
    "            balanced_X, balanced_y = balance_dataset(client_X, client_y, technique=balance)\n",
    "            return balanced_X, balanced_y\n",
    "        \n",
    "        def balance_thread(client_data, result, index):\n",
    "            result[index] = balance_client_data(client_data)\n",
    "\n",
    "        # Assuming client_raw_data is a list of data for each client\n",
    "        client_train = [None] * len(client_raw_data)  # Placeholder for results\n",
    "        threads = []\n",
    "\n",
    "        # Create and start threads\n",
    "        for i, client_data in enumerate(client_raw_data):\n",
    "            thread = threading.Thread(target=balance_thread, args=(client_data, client_train, i))\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        display_dataset_split(client_train, test_data)\n",
    "        \n",
    "        aggregated_model = eGAUSSp(**federated_model_params)\n",
    "        federated_model = eGAUSSp(**federated_model_params)\n",
    "\n",
    "        # Train local models\n",
    "        train_models_in_threads(local_models, client_train)\n",
    "        \n",
    "        '''\n",
    "        for local_model, client_data in zip(local_models, clients_data):\n",
    "             train_supervised(local_model, client_data)\n",
    "\n",
    "             print(f\"Number of local model clusters = {sum(local_model.n[0:local_model.c]> local_model.kappa_n)}\")\n",
    "             all_scores, pred_max, _ = test_model_in_batches(local_model, client_data)\n",
    "             binary = calculate_metrics(pred_max, client_data, \"binary\")\n",
    "             roc_auc = calculate_roc_auc(all_scores, client_data)\n",
    "             print(f\"Test Metrics: {binary}\")\n",
    "             print(f\"Test ROC AUC: {roc_auc}\")\n",
    "             plot_confusion_matrix(pred_max, client_data)\n",
    "        '''   \n",
    "\n",
    "        # Update federated model with local models\n",
    "        for client_idx, client_model in enumerate(local_models):\n",
    "\n",
    "            print(f\"Number of local model clusters = {sum(client_model.n[0:client_model.c]> 0)}\")\n",
    "            # Run the forward function on the training data\n",
    "            \n",
    "            \n",
    "            all_scores, pred_max, _ = test_model_in_batches(client_model, client_train[client_idx], batch_size=300)\n",
    "            binary = calculate_metrics(pred_max, client_train[client_idx], \"binary\")\n",
    "            roc_auc = calculate_roc_auc(all_scores, client_train[client_idx])\n",
    "            print(f\"Test Metrics: {binary}\")\n",
    "            print(f\"Test ROC AUC: {roc_auc}\")\n",
    "           # plot_confusion_matrix(pred_max, clients_data[client_idx])\n",
    "            \n",
    "\n",
    "            #client_model.federal_agent.federated_merging()\n",
    "            #print(f\"Number of local model clusters after merging = {sum(client_model.n[0:client_model.c]> client_model.kappa_n)}\")\n",
    "\n",
    "            #client_model.federal_agent.federated_merging()\n",
    "            print(f\"Updating agreggated model with client {client_idx + 1}\")\n",
    "\n",
    "            #client_model.federal_agent.federated_merging()\n",
    "            aggregated_model.federal_agent.merge_model_privately(client_model, client_model.kappa_n)\n",
    "            print(f\"Number of agreggated clusters after transfer = {sum(aggregated_model.n[0:aggregated_model.c]> aggregated_model.kappa_n)}\")\n",
    "                \n",
    "        aggregated_model.federal_agent.federated_merging()\n",
    "        print(f\"Number of agreggated clusters after merging = {sum(aggregated_model.n[0:aggregated_model.c]> aggregated_model.kappa_n)}\")\n",
    "\n",
    "                \n",
    "        #client_model.score = 0*client_model.score  \n",
    "        #aggregated_model.S_glo = client_model.S_glo\n",
    "        #aggregated_model.mu_glo = client_model.mu_glo     \n",
    "        \n",
    "        #if round>1:\n",
    "        #    with torch.no_grad():\n",
    "        #        aggregated_model.S = nn.Parameter(aggregated_model.S/2)\n",
    "        #        aggregated_model.n = nn.Parameter(aggregated_model.n/num_clients)\n",
    "\n",
    "        #        aggregated_model.S_glo = aggregated_model.S_glo/2\n",
    "        #        aggregated_model.n_glo = aggregated_model.n_glo/num_clients\n",
    "        \n",
    "\n",
    "        # New code for comparison using the updated compare_models function\n",
    "        #are_models_same, comparison_message = compare_models(client_model, aggregated_model)\n",
    "        #print(f\"Comparison details: {comparison_message}\")\n",
    "\n",
    "        # Update federated model with local models\n",
    "        print(f\"Updating federated model with agreggated model\")\n",
    "        federated_model = aggregated_model #.federal_agent.merge_model_privately(aggregated_model, federated_model.kappa_n)\n",
    "        print(f\"Number of federated clusters after transfer = {sum(federated_model.n[0:federated_model.c]> federated_model.kappa_n)}\")\n",
    "\n",
    "        #local_models = [eGAUSSp(**local_model_params) for _ in range(num_clients)]  \n",
    "        \n",
    "        # Perform federated merging and removal mechanism on the federated model\n",
    "        if any(federated_model.n[0:federated_model.c]> federated_model.kappa_n):\n",
    "\n",
    "            # Evaluate federated model\n",
    "            all_scores_fed, pred_max_fed, _ = test_model_in_batches(federated_model, test_data, batch_size=500)\n",
    "            binary_fed = calculate_metrics(pred_max_fed, test_data, \"binary\")\n",
    "            roc_auc_fed = calculate_roc_auc(all_scores_fed, test_data)\n",
    "            print(f\"Test Metrics: {binary_fed}\")\n",
    "            print(f\"Test ROC AUC: {roc_auc_fed}\")\n",
    "\n",
    "            #plot_confusion_matrix(pred_max_fed, test_data)\n",
    "\n",
    "            # Append metrics to the list\n",
    "            round_metrics.append({\n",
    "                'round': round + 1,\n",
    "                'clusters': federated_model.c,\n",
    "                'binary': binary_fed,\n",
    "                'roc_auc': roc_auc_fed\n",
    "            })\n",
    "\n",
    "        # Return the updated federated model to each client\n",
    "        for client_idx in range(len(local_models)):\n",
    "            print(f\"Returning updated model to client {client_idx + 1}\")\n",
    "            #local_models[client_idx] = federated_model\n",
    "            \n",
    "            local_models[client_idx].federal_agent.merge_model_privately(federated_model, federated_model.kappa_n)\n",
    "            local_models[client_idx].score = torch.ones_like(local_models[client_idx].score)\n",
    "            local_models[client_idx].num_pred = torch.zeros_like(local_models[client_idx].score)\n",
    "\n",
    "            #local_models[client_idx].federal_agent.federated_merging()\n",
    "            \n",
    "            '''\n",
    "            # Return the updated federated model to each client\n",
    "            for client_idx, client_model in enumerate(local_models):\n",
    "                print(f\"Returning updated model to client {client_idx + 1}\")\n",
    "                client_model.federal_agent.merge_model_privately(federated_model, federated_model.kappa_n)\n",
    "                client_model.federal_agent.federated_merging()\n",
    "            '''\n",
    "            \n",
    "        print(f\"--- End of Round {round + 1} ---\\n\")\n",
    "        #if  round == (num_rounds-1):\n",
    "        #    pass\n",
    "        plt.close('all')\n",
    "        #plot_interesting_features(client_train[0], model=federated_model, num_sigma=federated_model.num_sigma, N_max = federated_model.kappa_n)   \n",
    "        #plot_interesting_features(test_data, model=federated_model, num_sigma=federated_model.num_sigma, N_max = federated_model.kappa_n)   \n",
    "        #test_data, clients_data[0]\n",
    "        # Write round information and results to file\n",
    "        write_to_file(result_file, round_info)\n",
    "        for metric in round_metrics:\n",
    "            metric_info = f\"Round {metric['round']}: Metrics: {metric['binary']}, ROC AUC: {metric['roc_auc']}\\n\"\n",
    "            write_to_file(result_file, metric_info)\n",
    "\n",
    "        print(f\"--- End of Round {round + 1} ---\\n\")\n",
    "        round_info = f\"--- End of Round {round + 1} ---\\n\"\n",
    "        write_to_file(result_file, round_info)\n",
    "\n",
    "    # After all rounds\n",
    "    final_info = \"All Rounds Completed. Metrics Collected:\\n\"\n",
    "    write_to_file(result_file, final_info)\n",
    "    for metric in round_metrics:\n",
    "        final_metric_info = f\"Round {metric['round']}: Metrics: {metric['binary']}, ROC AUC: {metric['roc_auc']}\\n\"\n",
    "        print(final_metric_info)\n",
    "        write_to_file(result_file, final_metric_info)\n",
    "\n",
    "    return round_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of client counts and data configuration indices\n",
    "client_counts = [ 3 ,10]\n",
    "data_config_indices = [1]  # Replace with your actual data configuration indices\n",
    "\n",
    "# Assuming local_models, client_train, federated_model, and test_data are already defined\n",
    "# Number of communication rounds\n",
    "num_rounds = 100\n",
    "profiler = False\n",
    "experiments = []\n",
    "# Running the experiment\n",
    "for num_clients in client_counts:\n",
    "    for data_config_index in data_config_indices:\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        client_train, test_data, all_data = prepare_dataset(X, y, num_clients) \n",
    "        balance= None\n",
    "        if data_config_index == 1:\n",
    "\n",
    "            balance = 'random'\n",
    "                #'random': RandomUnderSampler(random_state=None),\n",
    "                #'tomek': TomekLinks(),\n",
    "                #'centroids': ClusterCentroids(random_state=None),\n",
    "                #'nearmiss': NearMiss(version=2),\n",
    "                #'enn': AllKNN(sampling_strategy='all'), NO\n",
    "                #'smote': SMOTE(random_state=None), NO\n",
    "                #'one_sided_selection': OneSidedSelection(random_state=None), NO\n",
    "                #'ncr': NeighbourhoodCleaningRule(), NO\n",
    "                #'function_sampler': FunctionSampler(),  # Identity resampler NO\n",
    "                #'instance_hardness_threshold': InstanceHardnessThreshold(estimator=LogisticRegression(), random_state=0),\n",
    "        elif data_config_index == 2:    \n",
    "             balance = 'Smote'       \n",
    "        elif data_config_index == 3:\n",
    "\n",
    "            balance = None\n",
    "        \n",
    "        print(f\"Running experiment with {num_clients} clients and data configuration {data_config_index}\")\n",
    "        if profiler:\n",
    "                        \n",
    "            import cProfile\n",
    "            %load_ext memory_profiler\n",
    "            import yappi\n",
    "\n",
    "            print(f\"... with profiler\")\n",
    "            pr = cProfile.Profile()\n",
    "            pr.enable()\n",
    "            yappi.start()\n",
    "            metrics = run_experiment(num_clients, num_rounds, client_train, test_data, balance)\n",
    "            yappi.stop()\n",
    "            pr.disable()\n",
    "\n",
    "            pr.print_stats(sort='cumtime')\n",
    "            yappi.get_thread_stats().print_all()\n",
    "            yappi.get_func_stats().print_all()   \n",
    "                   \n",
    "        else:\n",
    "            metrics = run_experiment(num_clients, num_rounds, client_train, test_data, balance)\n",
    "            experiments.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Rounds Completed. Metrics Collected:\")\n",
    "for metric in metrics:\n",
    "    print(f\"Round {metric['round']}: Metrics: {metric['binary']}, ROC AUC: {metric['roc_auc']}\")\n",
    "    print(f\"                         Weighted: {metric['weighted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for client_idx, client_model in enumerate(local_models):\n",
    "        print(f\"Merging client {client_idx + 1}\")\n",
    "        #print(f\"Number of client {client_idx + 1} clusters before merging = {torch.sum(client_model.n[:client_model.c]>client_model.kappa_n)}\")\n",
    "        #client_model.federal_agent.federated_merging() \n",
    "        print(f\"Number of client {client_idx + 1} after merging = {torch.sum(client_model.n[:client_model.c]>client_model.kappa_n)}\")\n",
    "        federated_model.federal_agent.merge_model_privately(client_model, client_model.kappa_n)\n",
    "\n",
    "print(f\"Number of clusters after transfer = {federated_model.c}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "federated_model.federal_agent.federated_merging()\n",
    "federated_model.removal_mech.removal_mechanism()\n",
    "print(f\"Number of clusters after merging = {federated_model.c}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "print(f\"\\nTesting federated model\")   \n",
    "\n",
    "all_scores, pred_max, _ = test_model(federated_model, test_data)\n",
    "metrics = calculate_metrics(pred_max, test_data, \"binary\")\n",
    "print(f\"Test Metrics: {metrics}\")\n",
    "roc_auc = calculate_roc_auc(all_scores, test_data)\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "\n",
    "plot_confusion_matrix(pred_max, test_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Confusion matrix values\n",
    "tn = 135\n",
    "fn = 10\n",
    "tp = 132\n",
    "fp = 19\n",
    "\n",
    "# Creating the confusion matrix\n",
    "y_true = [0]*tn + [1]*fn + [1]*tp + [0]*fp  # 0 for negative class, 1 for positive class\n",
    "y_pred = [0]*(tn+fn) + [1]*(fp+tp)  # Predictions\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(accuracy, precision, recall, f1)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
