{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.utils_train import train_supervised, train_models_in_threads, test_model_in_batches\n",
    "from utils.utils_plots import plot_first_feature\n",
    "from utils.utils_dataset import balance_dataset, prepare_dataset\n",
    "from utils.utils_dataset import prepare_non_iid_dataset, plot_dataset_split, display_dataset_split\n",
    "from utils.utils_metrics import calculate_metrics, plot_confusion_matrix, calculate_roc_auc\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.eGauss_plus import eGAUSSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Datasets/creditcard.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select the columns to normalize - all except 'Class'\n",
    "cols_to_normalize = [col for col in data.columns if col != 'Class']\n",
    "\n",
    "# Apply the normalization\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(f\"{torch.cuda.is_available()}\")\n",
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "local_model_params = {\n",
    "    \"feature_dim\": 30,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 1,\n",
    "    \"num_sigma\": 1,\n",
    "    \"kappa_join\": 0.8,\n",
    "    \"S_0\": 1e-2,\n",
    "    \"N_r\": 16,\n",
    "    \"c_max\": 300,\n",
    "    \"num_samples\": 200,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "federated_model_params = {\n",
    "    \"feature_dim\": 30,\n",
    "    \"num_classes\": 2,\n",
    "    \"kappa_n\": 1,\n",
    "    \"num_sigma\": 1,\n",
    "    \"kappa_join\": 0.8,\n",
    "    \"S_0\": 1e-2,\n",
    "    \"N_r\": 16,\n",
    "    \"c_max\": 300,\n",
    "    \"num_samples\": 200,\n",
    "    \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display_dataset_split(client_train, test_data)\n",
    "#plot_dataset_split(client_train, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cProfile\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2):\n",
    "    differences = []\n",
    "\n",
    "    # Function to find differing indices within the overlapping range\n",
    "    def find_differing_indices(tensor1, tensor2):\n",
    "        min_length = min(tensor1.size(0), tensor2.size(0))\n",
    "        differing = (tensor1[:min_length] != tensor2[:min_length]).nonzero(as_tuple=False)\n",
    "        if differing.nelement() == 0:\n",
    "            return \"No differences\"\n",
    "        else:\n",
    "            return differing.view(-1).tolist()  # Flatten and convert to list\n",
    "\n",
    "    # Compare mu parameter and find differing indices\n",
    "    mu_equal = torch.equal(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "    if not mu_equal:\n",
    "        differing_indices_mu = find_differing_indices(model1.mu[:model1.c], model2.mu[:model2.c])\n",
    "        differences.append(f\"mu parameter differs at indices {differing_indices_mu}\")\n",
    "\n",
    "    # Compare S parameter and find differing indices\n",
    "    S_equal = torch.equal(model1.S[:model1.c], model2.S[:model2.c])\n",
    "    if not S_equal:\n",
    "        differing_indices_S = find_differing_indices(model1.S[:model1.c], model2.S[:model2.c])\n",
    "        differences.append(f\"S parameter differs at indices {differing_indices_S}\")\n",
    "\n",
    "    # Compare n parameter and find differing indices\n",
    "    n_equal = torch.equal(model1.n[:model1.c], model2.n[:model2.c])\n",
    "    if not n_equal:\n",
    "        differing_indices_n = find_differing_indices(model1.n[:model1.c], model2.n[:model2.c])\n",
    "        differences.append(f\"n parameter differs at indices {differing_indices_n}\")\n",
    "\n",
    "    # Check if there are any differences\n",
    "    if differences:\n",
    "        difference_str = \", \".join(differences)\n",
    "        return False, f\"Differences found in: {difference_str}\"\n",
    "    else:\n",
    "        return True, \"Models are identical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def run_experiment(num_clients, num_rounds, clients_data, test_data):\n",
    "        \n",
    "    # Initialize a model for each client\n",
    "    local_models = [eGAUSSp(**local_model_params) for _ in range(num_clients)]\n",
    "    federated_model = eGAUSSp(**federated_model_params)\n",
    "\n",
    "    # Initialize a list to store the metrics for each round\n",
    "    round_metrics = []\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "        print(f\"--- Communication Round {round + 1} ---\")\n",
    "\n",
    "        aggregated_model = eGAUSSp(**federated_model_params)\n",
    "        federated_model = eGAUSSp(**federated_model_params)\n",
    "\n",
    "        # Train local models\n",
    "        #train_models_in_threads(local_models, clients_data)\n",
    "        for local_model, client_data in zip(local_models, clients_data):\n",
    "            train_supervised(local_model, client_data)\n",
    "            print(f\"Number of local model clusters = {sum(local_model.n[0:local_model.c]> local_model.kappa_n)}\")\n",
    "            all_scores, pred_max, _ = test_model_in_batches(local_model, client_data)\n",
    "            binary = calculate_metrics(pred_max, client_data, \"binary\")\n",
    "            roc_auc = calculate_roc_auc(all_scores, client_data)\n",
    "            print(f\"Test Metrics: {binary}\")\n",
    "            print(f\"Test ROC AUC: {roc_auc}\")\n",
    "            plot_confusion_matrix(pred_max, client_data)\n",
    "\n",
    "        # Update federated model with local models\n",
    "        for client_idx, client_model in enumerate(local_models):\n",
    "            #client_model.federal_agent.federated_merging()\n",
    "            print(f\"Updating agreggated model with client {client_idx + 1}\")\n",
    "        \n",
    "            client_model.federal_agent.federated_merging()\n",
    "            aggregated_model.federal_agent.merge_model_privately(client_model, client_model.kappa_n)\n",
    "            print(f\"Number of agreggated clusters after transfer = {sum(aggregated_model.n[0:aggregated_model.c]> aggregated_model.kappa_n)}\")\n",
    "        \n",
    "        aggregated_model.federal_agent.federated_merging()\n",
    "        print(f\"Number of agreggated clusters after merging = {sum(aggregated_model.n[0:aggregated_model.c]> aggregated_model.kappa_n)}\")\n",
    "\n",
    "                 \n",
    "        #client_model.score = 0*client_model.score  \n",
    "        #aggregated_model.S_glo = client_model.S_glo\n",
    "        #aggregated_model.mu_glo = client_model.mu_glo     \n",
    "        '''\n",
    "        if round>1:\n",
    "            with torch.no_grad():\n",
    "                aggregated_model.S = nn.Parameter(aggregated_model.S/2)\n",
    "                aggregated_model.n = nn.Parameter(aggregated_model.n/2)\n",
    "\n",
    "                aggregated_model.S_glo = aggregated_model.S_glo/2\n",
    "                aggregated_model.n_glo = aggregated_model.n_glo/2\n",
    "        '''\n",
    "\n",
    "        # New code for comparison using the updated compare_models function\n",
    "        #are_models_same, comparison_message = compare_models(client_model, aggregated_model)\n",
    "        #print(f\"Comparison details: {comparison_message}\")\n",
    "\n",
    "        # Update federated model with local models\n",
    "        print(f\"Updating federated model with agreggated model\")\n",
    "        federated_model = aggregated_model #.federal_agent.merge_model_privately(aggregated_model, federated_model.kappa_n)\n",
    "        print(f\"Number of federated clusters after transfer = {sum(federated_model.n[0:federated_model.c]> federated_model.kappa_n)}\")\n",
    "\n",
    " \n",
    "        # Perform federated merging and removal mechanism on the federated model\n",
    "        if any(federated_model.n[0:federated_model.c]> federated_model.kappa_n):\n",
    "\n",
    "            # Evaluate federated model\n",
    "            all_scores_fed, pred_max_fed, _ = test_model_in_batches(federated_model, test_data)\n",
    "            binary_fed = calculate_metrics(pred_max_fed, test_data, \"binary\")\n",
    "            roc_auc_fed = calculate_roc_auc(all_scores_fed, test_data)\n",
    "            print(f\"Test Metrics: {binary_fed}\")\n",
    "            print(f\"Test ROC AUC: {roc_auc_fed}\")\n",
    "\n",
    "            plot_confusion_matrix(pred_max_fed, test_data)\n",
    "\n",
    "            # Append metrics to the list\n",
    "            round_metrics.append({\n",
    "                'round': round + 1,\n",
    "                'clusters': federated_model.c,\n",
    "                'binary': binary_fed,\n",
    "                'roc_auc': roc_auc_fed\n",
    "            })\n",
    "            \n",
    "        local_models = [eGAUSSp(**local_model_params) for _ in range(num_clients)]  \n",
    "        # Return the updated federated model to each client\n",
    "        for client_idx in range(len(local_models)):\n",
    "            print(f\"Returning updated model to client {client_idx + 1}\")\n",
    "            #local_models[client_idx] = federated_model\n",
    "            \n",
    "            local_models[client_idx].federal_agent.merge_model_privately(federated_model, federated_model.kappa_n)\n",
    "            local_models[client_idx].score = torch.zeros_like(local_models[client_idx].score)\n",
    "\n",
    "            '''\n",
    "            # Return the updated federated model to each client\n",
    "            for client_idx, client_model in enumerate(local_models):\n",
    "                print(f\"Returning updated model to client {client_idx + 1}\")\n",
    "                client_model.federal_agent.merge_model_privately(federated_model, federated_model.kappa_n)\n",
    "                client_model.federal_agent.federated_merging()\n",
    "            '''\n",
    "            \n",
    "        print(f\"--- End of Round {round + 1} ---\\n\")\n",
    "        if  round == (num_rounds-1):\n",
    "            plot_first_feature(test_data, model=federated_model, num_sigma=2, N_max = federated_model.kappa_n)   \n",
    "    \n",
    "    # After all rounds\n",
    "    print(\"All Rounds Completed. Metrics Collected:\")\n",
    "    for metric in round_metrics:\n",
    "        print(f\"Round {metric['round']}: Metrics: {metric['binary']}, ROC AUC: {metric['roc_auc']}\")\n",
    "        #print(f\"                         Weighted: {metric['weighted']}\")\n",
    "\n",
    "\n",
    "    return round_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored When destroying _lsprof profiler:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mihao\\AppData\\Local\\Temp\\ipykernel_21484\\2913773090.py\", line 32, in <module>\n",
      "    pr.enable()\n",
      "RuntimeError: Cannot install a profile function while another profile function is being installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: {0: 1000, 1: 1000}\n",
      "Client 2: {0: 1000, 1: 1000}\n",
      "Client 3: {0: 1000, 1: 1000}\n",
      "Test Set: {0: 56864, 1: 98}\n",
      "\n",
      "Combined Number of Samples per Class:\n",
      "Class 0: 59864 samples\n",
      "Class 1: 3098 samples\n",
      "\n",
      "Total Number of Samples Across All Datasets: 62962\n",
      "Running experiment with 3 clients and data configuration 1\n",
      "... with profiler\n",
      "--- Communication Round 1 ---\n",
      "Evolving has been enabled.\n",
      "Number of local model clusters = 370\n",
      "Evolving has been disabled.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m pr \u001b[38;5;241m=\u001b[39m cProfile\u001b[38;5;241m.\u001b[39mProfile()\n\u001b[0;32m     32\u001b[0m pr\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m  \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m pr\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m     35\u001b[0m pr\u001b[38;5;241m.\u001b[39mprint_stats(sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(num_clients, num_rounds, clients_data, test_data)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of local model clusters = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(local_model\u001b[38;5;241m.\u001b[39mn[\u001b[38;5;241m0\u001b[39m:local_model\u001b[38;5;241m.\u001b[39mc]\u001b[38;5;241m>\u001b[39m\u001b[38;5;250m \u001b[39mlocal_model\u001b[38;5;241m.\u001b[39mkappa_n)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m all_scores, pred_max, _ \u001b[38;5;241m=\u001b[39m test_model_in_batches(local_model, client_data)\n\u001b[1;32m---> 24\u001b[0m binary \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m calculate_roc_auc(all_scores, client_data)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbinary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\eGAUSSp_Python\\utils\\utils_metrics.py:17\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(pred_max, test_dataset, weight)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_metrics\u001b[39m(pred_max, test_dataset, weight):\n\u001b[0;32m     15\u001b[0m     _, test_labels \u001b[38;5;241m=\u001b[39m test_dataset\n\u001b[1;32m---> 17\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     precision \u001b[38;5;241m=\u001b[39m precision_score(test_labels, pred_max, average\u001b[38;5;241m=\u001b[39mweight, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     recall \u001b[38;5;241m=\u001b[39m recall_score(test_labels, pred_max, average\u001b[38;5;241m=\u001b[39mweight, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\eGAUSSp_Python\\.conda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mihao\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\eGAUSSp_Python\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mihao\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\eGAUSSp_Python\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "# List of client counts and data configuration indices\n",
    "client_counts = [3, 10]\n",
    "data_config_indices = [1, 3, 1]  # Replace with your actual data configuration indices\n",
    "\n",
    "# Assuming local_models, client_train, federated_model, and test_data are already defined\n",
    "# Number of communication rounds\n",
    "num_rounds = 1\n",
    "profiler = True\n",
    "experiments = []\n",
    "# Running the experiment\n",
    "for num_clients in client_counts:\n",
    "    for data_config_index in data_config_indices:\n",
    "        if data_config_index == 1:\n",
    "            X = data.iloc[:, :-1].values\n",
    "            y = data.iloc[:, -1].values\n",
    "            client_train, test_data, all_data = prepare_dataset(X, y, num_clients, balance = 300) \n",
    "            #'random', 'centroids', 'nearmiss', 'enn', 'smote', int num of samples\n",
    "            \n",
    "        if data_config_index == 3:\n",
    "            X = data.iloc[:, :-1].values\n",
    "            y = data.iloc[:, -1].values\n",
    "            client_train, test_data, all_data = prepare_dataset(X, y, num_clients,balance = 'smote') \n",
    "\n",
    "        display_dataset_split(client_train, test_data)\n",
    "        \n",
    "        print(f\"Running experiment with {num_clients} clients and data configuration {data_config_index}\")\n",
    "        if profiler:\n",
    "            print(f\"... with profiler\")\n",
    "            # Assuming models and client_data are already defined\n",
    "            #for client_idx, client_model in enumerate(local_models)\n",
    "            pr = cProfile.Profile()\n",
    "            pr.enable()\n",
    "            metrics =  run_experiment(num_clients, num_rounds, client_train, test_data)\n",
    "            pr.disable()\n",
    "            pr.print_stats(sort='cumtime')\n",
    "            \n",
    "        else:\n",
    "            metrics = run_experiment(num_clients, num_rounds, client_train, test_data)\n",
    "            experiments.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Rounds Completed. Metrics Collected:\")\n",
    "for metric in metrics:\n",
    "    print(f\"Round {metric['round']}: Metrics: {metric['binary']}, ROC AUC: {metric['roc_auc']}\")\n",
    "    print(f\"                         Weighted: {metric['weighted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for client_idx, client_model in enumerate(local_models):\n",
    "        print(f\"Merging client {client_idx + 1}\")\n",
    "        #print(f\"Number of client {client_idx + 1} clusters before merging = {torch.sum(client_model.n[:client_model.c]>client_model.kappa_n)}\")\n",
    "        #client_model.federal_agent.federated_merging() \n",
    "        print(f\"Number of client {client_idx + 1} after merging = {torch.sum(client_model.n[:client_model.c]>client_model.kappa_n)}\")\n",
    "        federated_model.federal_agent.merge_model_privately(client_model, client_model.kappa_n)\n",
    "\n",
    "print(f\"Number of clusters after transfer = {federated_model.c}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "federated_model.federal_agent.federated_merging()\n",
    "federated_model.removal_mech.removal_mechanism()\n",
    "print(f\"Number of clusters after merging = {federated_model.c}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "print(f\"\\nTesting federated model\")   \n",
    "\n",
    "all_scores, pred_max, _ = test_model(federated_model, test_data)\n",
    "metrics = calculate_metrics(pred_max, test_data, \"binary\")\n",
    "print(f\"Test Metrics: {metrics}\")\n",
    "roc_auc = calculate_roc_auc(all_scores, test_data)\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "\n",
    "plot_confusion_matrix(pred_max, test_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Confusion matrix values\n",
    "tn = 135\n",
    "fn = 10\n",
    "tp = 132\n",
    "fp = 19\n",
    "\n",
    "# Creating the confusion matrix\n",
    "y_true = [0]*tn + [1]*fn + [1]*tp + [0]*fp  # 0 for negative class, 1 for positive class\n",
    "y_pred = [0]*(tn+fn) + [1]*(fp+tp)  # Predictions\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(accuracy, precision, recall, f1)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
